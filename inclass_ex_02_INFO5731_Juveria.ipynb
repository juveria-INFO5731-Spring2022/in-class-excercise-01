{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inclass-ex-02 INFO5731 Juveria",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXcYs7yMItEXdZ2h5TYhJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juveria-INFO5731-Spring2022/in-class-excercise-01/blob/main/inclass_ex_02_INFO5731_Juveria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CwhI9RVZBhY"
      },
      "outputs": [],
      "source": [
        "#Describe an interesting research question (or practical question)\n",
        "#In today's world of fake identities it is more important to collect the data of candidates who is getting hired.  In my point most of the data should be collected to review the past details of employees for future refrences. The data that should be collected from candidates is their \n",
        "#Social Security , government id proofs and any future work records regarding security as it has become a important step to follow before hiring. After collecting data it should be saved in a file for \n",
        "#future works whenever there is a need to use that data. In most of the situation absence of data may lead to serious problems for company's if something unwanted happens so, it is necessary\n",
        "#to have data of everything.\n",
        "\n",
        "#part2 Write python code to collect 1000 data samples\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"employees.csv\")\n",
        "rows = data.sample(frac =.25)\n",
        "if (0.25*(len(data))== len(rows)):\n",
        "    print( \"Cool\")\n",
        "    print(len(data), len(rows))\n",
        "rows\n",
        "\n",
        "\n",
        "#part3 #1000 posts collection from twitter\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "def printtweetdata(n, ith_tweet):\n",
        "        print()\n",
        "        print(f\"Tweet {n}:\")\n",
        "        print(f\"Username:{ith_tweet[0]}\")\n",
        "        print(f\"Description:{ith_tweet[1]}\")\n",
        "        print(f\"Tweet Text:{ith_tweet[2]}\")\n",
        "        print(f\"Hashtags Used:{ith_tweet[3]}\")\n",
        "def scrape(words, date_since, numtweet):\n",
        "        db = pd.DataFrame(columns=['username',\n",
        "                                   'description',\n",
        "                                   'text'])\n",
        "                                  \n",
        " \n",
        "        tweets = tweepy.Cursor(api.search_tweets,\n",
        "                               words, lang=\"en\",\n",
        "                               since_id=date_since,\n",
        "                               tweet_mode='extended').items(numtweet)\n",
        " \n",
        " \n",
        "        \n",
        "        list_tweets = [tweet for tweet in tweets]\n",
        " \n",
        "        \n",
        "        i = 1\n",
        "        for tweet in list_tweets:\n",
        "                username = tweet.user.screen_name\n",
        "                description = tweet.user.description\n",
        "                hashtags = tweet.entities['hashtags']\n",
        "                try:\n",
        "                        text = tweet.retweeted_status.full_text\n",
        "                except AttributeError:\n",
        "                        text = tweet.full_text\n",
        "                hashtext = list()\n",
        "                for j in range(0, len(hashtags)):\n",
        "                        hashtext.append(hashtags[j]['text'])\n",
        " \n",
        "                ith_tweet = [username,time,\n",
        "                             text]\n",
        "                db.loc[len(db)] = ith_tweet\n",
        " \n",
        "               \n",
        "                printtweetdata(i, ith_tweet)\n",
        "                i = i+1\n",
        "        filename = 'scraped_tweets.csv'\n",
        " \n",
        "        \n",
        "        db.to_csv(filename)\n",
        " \n",
        "if __name__ == '__main__':\n",
        " \n",
        "        consumer_key = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "        consumer_secret = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "        access_key = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        "        access_secret = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
        " \n",
        " \n",
        "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "        auth.set_access_token(access_key, access_secret)\n",
        "        api = tweepy.API(auth)\n",
        " \n",
        "        # Enter Hashtag and initial date\n",
        "        print(\"Enter Twitter HashTag to search for\")\n",
        "        words = input()\n",
        "        print(\"Enter Date since The Tweets are required in yyyy-mm--dd\")\n",
        "        date_since = input()\n",
        " \n",
        "        # number of tweets you want to extract in one run\n",
        "        numtweet = 1000\n",
        "        scrape(words, date_since, numtweet)\n",
        "        print('Scraping has completed!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JqGGnBQp8Xyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}